#+TITLE: Legal RAG System Implementation
#+AUTHOR: DefRecord Team
#+DATE: 2025-03-23
#+PROPERTY: header-args:hy :tangle yes :mkdirp yes
#+OPTIONS: ^:nil

* Legal RAG System

This document contains the literate programming implementation of the Legal RAG System. The code is organized in a modular fashion to promote maintainability and ease of understanding.

** System Overview

The Legal RAG System consists of the following main components:

1. Core RAG System - Provides the main interface for queries
2. Vector Search - Retrieves relevant documents
3. Context Formation - Formats retrieved documents for LLM
4. Cache Management - Optimizes performance 
5. Jurisdiction Handling - Implements legal hierarchy awareness
6. Citation Management - Handles legal citations

Let's implement each component.

** Core System Implementation
:PROPERTIES:
:header-args:hy: :tangle src/legal_rag/system.hy
:END:

*** Imports

#+begin_src hy
#!/usr/bin/env hy

(import
  os
  json
  [datetime [datetime timedelta]]
  [typing [Dict List Any Optional Tuple]]
  [functools [lru_cache]]
  [dataclasses [dataclass]]
  
  ;; Vector database and embeddings
  numpy :as np
  faiss
  
  ;; LLM client
  openai
  
  ;; Local imports
  [.cache [EmbeddingCache]]
  [.citation [format-citation parse-citation]]
  [.jurisdiction [get-jurisdiction-boost calculate-precedential-value]]
  [.evaluation [evaluate-response]]
  [.utils [retry_with_exponential_backoff timeout]])

;; Configure API key
(setv openai.api_key (os.getenv "OPENAI_API_KEY"))
#+end_src

*** Response Dataclass

#+begin_src hy
;; Response dataclass
(dataclass
  (defclass RAGResponse []
    "Response from the Legal RAG system"
    (^str answer None)
    (^List citations [])
    (^Dict metadata {})
    (^List retrieved-documents [])
    (^float confidence 0.0)
    (^Dict evaluation {})))
#+end_src

*** Main System Class

#+begin_src hy
;; Main RAG System
(defclass LegalRAGSystem []
  "Jurisdiction-aware legal RAG system"
  
  (defn __init__ [self
                 &optional
                 [^str embedding-model "text-embedding-3-large"]
                 [^str llm-model "gpt-4o"]
                 [^str jurisdiction "US-FED"]
                 [^int max-tokens 1024]
                 [^bool use-cache True]
                 [^int cache-ttl 86400]  ; 24 hours
                 [^str index-path None]]
    "Initialize the Legal RAG system"
    (setv self.embedding-model embedding-model)
    (setv self.llm-model llm-model)
    (setv self.jurisdiction jurisdiction)
    (setv self.max-tokens max-tokens)
    (setv self.use-cache use-cache)
    
    ;; Set up embedding cache if enabled
    (when use-cache
      (setv self.embedding-cache (EmbeddingCache :ttl cache-ttl)))
    
    ;; Load vector index
    (self.load-index index-path))
#+end_src

*** Index Management

#+begin_src hy  
  (defn load-index [self &optional [index-path None]]
    "Load the FAISS vector index from disk or initialize a new one"
    (if (and index-path (os.path.exists index-path))
        ;; Load existing index
        (do
          (setv self.index (faiss.read_index index-path))
          (with [f (open f"{index-path}.meta" "r")]
            (setv self.document-store (json.load f))))
        
        ;; Initialize new index
        (do
          (setv self.document-store {})
          (setv dimension 1536)  ; Default for OpenAI embeddings
          (setv self.index (faiss.IndexFlatL2 dimension)))))
  
  (defn save-index [self index-path]
    "Save the FAISS vector index to disk"
    (faiss.write_index self.index index-path)
    (with [f (open f"{index-path}.meta" "w")]
      (json.dump self.document-store f)))
  
  (defn add-document [self document]
    "Add a document to the vector index"
    (let [text (get document "text")
          metadata (get document "metadata")
          
          ;; Generate embedding
          embedding (self.generate-embedding text)
          
          ;; Add to FAISS index
          document-id (len self.document-store)
          _ (self.index.add (np.array [embedding] :dtype np.float32))
          
          ;; Add to document store
          document-with-embedding (.copy document)
          _ (setv (get document-with-embedding "embedding") embedding)
          _ (setv (get self.document-store (str document-id)) document-with-embedding)]
      
      document-id))
  
  (defn bulk-add-documents [self documents]
    "Add multiple documents to the vector index"
    (let [document-ids (lfor doc documents
                             (self.add-document doc))]
      document-ids))
#+end_src

*** Embedding Generation

#+begin_src hy
  @(retry_with_exponential_backoff
    :initial-delay 1
    :max-retries 3
    :factor 2)
  @(timeout :seconds 10)
  (defn generate-embedding [self text]
    "Generate embedding for text using OpenAI API with caching"
    
    (if (and self.use-cache (hasattr self "embedding-cache"))
        ;; Try to get from cache first
        (let [cached-embedding (self.embedding-cache.get text)]
          (if cached-embedding
              cached-embedding
              ;; Not in cache, generate and store
              (let [embedding (self._generate_embedding_api text)]
                (self.embedding-cache.set text embedding)
                embedding)))
        
        ;; No cache, directly call API
        (self._generate_embedding_api text)))
  
  (defn _generate-embedding-api [self text]
    "Generate embedding via API call"
    (let [response (openai.embeddings.create
                     :model self.embedding-model
                     :input text)
          embedding (get (get response "data" [{}]) 0 {})
          embedding-vector (get embedding "embedding" [])]
      
      (np.array embedding-vector :dtype np.float32)))
#+end_src

*** Vector Search and Similarity

#+begin_src hy
  (defn search-similar-documents [self query &optional [k 5]]
    "Find k most similar documents to query"
    (let [query-embedding (self.generate-embedding query)
          
          ;; Custom similarity function with jurisdiction boost
          similarity-fn (fn [doc-id doc-embedding]
                          (let [doc (get self.document-store (str doc-id))
                                metadata (get doc "metadata")
                                base-score (self.cosine-similarity 
                                             query-embedding 
                                             doc-embedding)
                                jurisdiction-boost (get-jurisdiction-boost 
                                                     self.jurisdiction
                                                     (get metadata "jurisdiction" "US-FED"))]
                            (* base-score jurisdiction-boost)))
          
          ;; Perform search
          [distances indices] (self.index.search (np.array [query-embedding] :dtype np.float32) (* k 2))
          
          ;; Get documents and calculate custom scores
          results []
          _ (for [idx indices[0]]
              (when (>= idx 0)
                (let [doc-id (str idx)
                      document (get self.document-store doc-id)
                      embedding (get document "embedding")
                      score (similarity-fn idx embedding)]
                  
                  (.append results {"document" document
                                   "score" score}))))
          
          ;; Sort by custom score and take top k
          sorted-results (sorted results :key (fn [r] (- (get r "score"))))
          top-k-results (cut sorted-results 0 k)]
      
      top-k-results))
  
  (defn cosine-similarity [self v1 v2]
    "Calculate cosine similarity between two vectors"
    (let [dot-product (np.dot v1 v2)
          norm-v1 (np.linalg.norm v1)
          norm-v2 (np.linalg.norm v2)]
      
      (/ dot-product (* norm-v1 norm-v2))))
#+end_src

*** Context Formation

#+begin_src hy
  (defn format-context [self query results]
    "Format retrieved documents into LLM context"
    (let [;; Extract document content with metadata
          documents (lfor result results
                         (let [doc (get result "document")
                               text (get doc "text")
                               meta (get doc "metadata")
                               citation (format-citation meta)
                               score (get result "score")]
                           {"content" text
                            "citation" citation
                            "relevance" score}))
          
          ;; Order by relevance and create context string
          context-parts (lfor [i doc] (enumerate documents)
                             (format "Document #{(+ i 1)}: {(get doc \"content\")}\n"
                                    "Citation: {(get doc \"citation\")}\n"
                                    "Relevance: {(get doc \"relevance\"):.4f}\n\n"))
          
          ;; Combine with query
          system-context (format "Use these documents to answer the legal query.\n"
                                "Always cite specific documents in your answer.\n"
                                "If documents are insufficient, state this clearly.\n\n"
                                "Query: {query}\n\n"
                                "Retrieved documents:\n{(.join \"\" context-parts)}"))]
      
      system-context))
#+end_src

*** Answer Generation

#+begin_src hy
  @(retry_with_exponential_backoff
    :initial-delay 1
    :max-retries 3
    :factor 2)
  @(timeout :seconds 30)
  (defn generate-answer [self context]
    "Generate answer using LLM"
    (let [messages [{"role" "system"
                     "content" "You are a legal research assistant with expertise in case law and legislation. Provide accurate, well-cited answers."}
                    {"role" "user"
                     "content" context}]
          
          response (openai.chat.completions.create
                     :model self.llm-model
                     :messages messages
                     :max_tokens self.max-tokens
                     :temperature 0.2)
          
          answer (get (get response "choices" [{}]) 0 {})
          answer-text (get (get answer "message" {}) "content" "")
          
          ;; Extract citations from the response
          citations (parse-citation answer-text)]
      
      {"answer" answer-text
       "citations" citations}))
#+end_src

*** Query Pipeline

#+begin_src hy
  (defn query [self query]
    "Full query pipeline"
    (let [;; Retrieve similar documents
          results (self.search-similar-documents query)
          
          ;; Format context
          context (self.format-context query results)
          
          ;; Generate answer
          generated-result (self.generate-answer context)
          
          answer (get generated-result "answer")
          citations (get generated-result "citations")
          
          ;; Evaluate response
          evaluation (evaluate-response 
                      query 
                      answer 
                      citations 
                      results)
          
          ;; Calculate confidence score
          avg-relevance (/ (sum (lfor r results (get r "score"))) (max 1 (len results)))
          confidence (min 1.0 (* 0.7 avg-relevance (get evaluation "accuracy" 0.5)))
          
          ;; Create response object
          response (RAGResponse
                    :answer answer
                    :citations citations
                    :metadata {"query" query
                              "jurisdiction" self.jurisdiction
                              "timestamp" (str (datetime.now))}
                    :retrieved-documents results
                    :confidence confidence
                    :evaluation evaluation)]
      
      response)))
#+end_src

** Cache Implementation
:PROPERTIES:
:header-args:hy: :tangle src/legal_rag/cache.hy
:END:

The caching system optimizes API usage by storing embeddings with TTL.

#+begin_src hy
#!/usr/bin/env hy

(import
  os
  pickle
  [datetime [datetime timedelta]]
  [typing [Dict Any Optional]]
  [pathlib [Path]]
  [hashlib [md5]])

(defclass EmbeddingCache []
  "Cache for embeddings with TTL"
  
  (defn __init__ [self &optional 
                 [^int ttl 86400]  ; Default 24 hours in seconds
                 [^str cache-dir "cache"]
                 [^str version "v1"]]
    "Initialize embedding cache"
    (setv self.ttl ttl)
    (setv self.version version)
    
    ;; Create cache directory if it doesn't exist
    (setv self.cache-dir (os.path.join (os.path.dirname (os.path.abspath __file__)) cache-dir))
    (os.makedirs self.cache-dir :exist_ok True)
    
    ;; Load cache if it exists
    (setv self.cache-path (os.path.join self.cache-dir f"embedding_cache_{version}.pkl"))
    
    (if (os.path.exists self.cache-path)
        (self.load-cache)
        (setv self.cache {})))
  
  (defn load-cache [self]
    "Load cache from disk"
    (try
      (with [f (open self.cache-path "rb")]
        (setv self.cache (pickle.load f)))
      (except [e Exception]
        (print f"Error loading cache: {e}")
        (setv self.cache {}))))
  
  (defn save-cache [self]
    "Save cache to disk"
    (try
      (with [f (open self.cache-path "wb")]
        (pickle.dump self.cache f))
      (except [e Exception]
        (print f"Error saving cache: {e}"))))
  
  (defn get-key [self text]
    "Generate a unique key for the text"
    (let [hash-obj (md5 (.encode text "utf-8"))]
      (.hexdigest hash-obj)))
  
  (defn get [self text]
    "Get embedding from cache if it exists and is not expired"
    (let [key (self.get-key text)]
      (when (in key self.cache)
        (let [entry (get self.cache key)
              created-at (get entry "created_at")
              embedding (get entry "embedding")
              
              ;; Check if entry is expired
              now (datetime.now)
              expiry-time (+ created-at (timedelta :seconds self.ttl))]
          
          (if (< now expiry-time)
              ;; Valid cache entry
              embedding
              ;; Expired, remove from cache
              (do
                (del (get self.cache key))
                None))))))
  
  (defn set [self text embedding]
    "Store embedding in cache"
    (let [key (self.get-key text)]
      (setv (get self.cache key) {"embedding" embedding
                                  "created_at" (datetime.now)})
      
      ;; Save cache to disk (could be optimized to save less frequently)
      (self.save-cache)
      
      embedding))
  
  (defn clear [self]
    "Clear the entire cache"
    (setv self.cache {})
    (self.save-cache))
  
  (defn clean-expired [self]
    "Remove expired entries from cache"
    (let [now (datetime.now)
          expired-keys []]
      
      ;; Find expired keys
      (for [[key entry] (.items self.cache)]
        (let [created-at (get entry "created_at")
              expiry-time (+ created-at (timedelta :seconds self.ttl))]
          (when (>= now expiry-time)
            (.append expired-keys key))))
      
      ;; Remove expired entries
      (for [key expired-keys]
        (del (get self.cache key)))
      
      (when expired-keys
        (self.save-cache))
      
      (len expired-keys))))
#+end_src

** Demo Application
:PROPERTIES:
:header-args:hy: :tangle src/legal_rag/demo.hy
:END:

Let's create a simple demo application that shows the system in action.

#+begin_src hy
#!/usr/bin/env hy

(import sys
        os
        numpy :as np
        [.system [LegalRAGSystem]]
        [.cache [EmbeddingCache]])

(defn generate-sample-documents []
  "Generate sample legal documents for demo"
  [
    ;; Supreme Court case
    {"text" "The fair use doctrine is a legal principle that permits limited use of copyrighted material without acquiring permission from the rights holders. It is an exception to the exclusive rights granted by copyright law to the author of a creative work. In Campbell v. Acuff-Rose Music, Inc., the Supreme Court recognized that the fair use doctrine is an \"equitable rule of reason\" which permits courts to avoid rigid application of the copyright statute when it would stifle the very creativity that law is designed to foster."
     "metadata" {"case_name" "Campbell v. Acuff-Rose Music, Inc."
                "court" "Supreme Court"
                "citation" "510 U.S. 569"
                "year" "1994"
                "jurisdiction" "US-SCOTUS"
                "practice_area" "Intellectual Property"}}
    
    ;; 9th Circuit case
    {"text" "In the Ninth Circuit's decision in Perfect 10 v. Amazon, the court held that search engines' creation and display of thumbnail images constituted fair use. The court emphasized that the use was highly transformative because the thumbnails served a different function than the original images. This transformative nature was sufficient to outweigh the commercial aspects of Google's use. The court also noted that the thumbnails did not harm the potential market for the original images because they were not substitutes for the full-sized images."
     "metadata" {"case_name" "Perfect 10, Inc. v. Amazon.com, Inc."
                "court" "9th Circuit"
                "citation" "508 F.3d 1146"
                "year" "2007"
                "jurisdiction" "US-9CIR"
                "practice_area" "Intellectual Property"}}
    
    ;; California case
    {"text" "Under California law, determining whether a use of copyrighted material constitutes fair use requires an analysis of four factors: (1) the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes; (2) the nature of the copyrighted work; (3) the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and (4) the effect of the use upon the potential market for or value of the copyrighted work. These factors should not be treated in isolation but weighed together in light of the purposes of copyright."
     "metadata" {"case_name" "Zucker v. Los Angeles Times"
                "court" "California Supreme Court"
                "citation" "18 Cal.4th 168"
                "year" "2005"
                "jurisdiction" "CA-SUP"
                "practice_area" "Intellectual Property"}}
    
    ;; Explanatory document
    {"text" "Fair use is determined on a case-by-case basis, and all four factors must be weighed together. Transformative uses, which add something new or serve a different purpose than the original work, are more likely to be considered fair use. Examples of transformative uses include parody, criticism, commentary, news reporting, teaching, scholarship, and research. However, commercial uses are less likely to be considered fair use, though this is not determinative. The second factor considers whether the original work is creative or factual, with greater protection given to creative works. The third factor examines how much of the original work was used, both quantitatively and qualitatively. The fourth factor assesses market harm to the original work."
     "metadata" {"title" "Fair Use Doctrine Explanation"
                "author" "Legal Research Institute"
                "year" "2023"
                "jurisdiction" "US-FED"
                "practice_area" "Intellectual Property"}}
  ])

(defn demo []
  "Run an interactive demo of the Legal RAG system"
  (print "\nLegal RAG System Demo")
  (print "=====================\n")
  
  ;; Create system
  (setv rag-system (LegalRAGSystem 
                    :embedding-model "text-embedding-3-large"
                    :llm-model "gpt-4o"
                    :jurisdiction "CA-9"))
  
  ;; Add sample documents
  (setv sample-docs (generate-sample-documents))
  (rag-system.bulk-add-documents sample-docs)
  (print f"Added {(len sample-docs)} sample documents to the system.\n")
  
  ;; Interactive query loop
  (print "Enter legal queries about fair use (or 'exit' to quit):")
  (while True
    (print "\nQuery> " :end "")
    (sys.stdout.flush)
    (setv query (.strip (input)))
    
    (when (or (= query "exit") (= query "quit"))
      (break))
    
    (when (< (len query) 5)
      (print "Query too short. Please enter a more detailed question.")
      (continue))
    
    (print "\nProcessing query...")
    (setv response (rag-system.query query))
    
    (print "\n" (+ "=" (* 80 "-")))
    (print response.answer)
    (print (+ "=" (* 80 "-")))
    
    (print "\nCitations:")
    (for [citation response.citations]
      (print f"- {(get citation \"citation\")}"))
    
    (print f"\nConfidence: {(* 100 response.confidence):.1f}%")
    
    (print "\nRetrieved documents:")
    (for [[i doc] (enumerate response.retrieved-documents)]
      (setv citation (format-citation (get (get doc "document") "metadata")))
      (print f"{(+ i 1)}. {citation} (Score: {(get doc \"score\"):.4f})")))
  
  (print "\nThank you for using the Legal RAG System!"))

(when (= __name__ "__main__")
  (demo))
#+end_src

** Package Initialization
:PROPERTIES:
:header-args:hy: :tangle src/legal_rag/__init__.hy
:END:

Finally, let's define the public API for our package:

#+begin_src hy
#!/usr/bin/env hy

(import [.system [LegalRAGSystem RAGResponse]]
        [.citation [format-citation parse-citation]]
        [.evaluation [evaluate-response]]
        [.jurisdiction [get-jurisdiction-boost calculate-precedential-value]])

__version__ = "0.1.0"
__author__ = "DefRecord Team <info@defrecord.com>"
__license__ = "MIT"

;; Export public API
__all__ = ["LegalRAGSystem", "RAGResponse", 
          "format-citation", "parse-citation",
          "evaluate-response", "get-jurisdiction-boost"]
#+end_src

* Conclusion

This document provided the literate programming implementation of the Legal RAG System. The system combines several components to create a jurisdiction-aware retrieval-augmented generation system for legal research.

The implementation showcases functional programming patterns in Hy, a Lisp dialect embedded in Python, to create a modular and maintainable system.